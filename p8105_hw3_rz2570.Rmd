---
title: "p8105_hw3_rz2570"
author: "Ruilian Zhang"
date: "10/12/2021"
output: github_document
---

```{r}
library(tidyverse)
library(p8105.datasets)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


# Problem 1

```{r}
# Import data and do some exploration.

data("instacart")

dim(instacart)

names(instacart)

head(instacart) 
```

* The `instacart` dataframe has `r ncol(instacart)` of columns and `r nrow(instacart)` of observations. Each row contains relative information of an item from one order , which are: `r names(instacart)`.  
* For example, for product `49302`, its order of being added to cart was `1` by user `112108`, and it's from `dairy eggs` department.  

```{r}
# How many aisles are there, and which aisles are the most items ordered from?

ins_aisle = instacart %>% 
  group_by(aisle) %>% 
  summarize(
    n_obs = n()) %>% 
  arrange(-n_obs)
```

* There are `r nrow(ins_aisle)` aisles, the `fresh vegetables`  (`aisle_id` is 83) aisle is the most items are ordered from.

```{r}
# Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered.

ins_aisle %>% 
  filter(n_obs > 10000) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n_obs)) %>% 
  ggplot(aes(x = aisle, y = n_obs)) +
  geom_bar(stat = "identity", fill = "blue", alpha = .8) +
  coord_flip() +
  labs(
    title = "The number of items ordered in each aisle",
    x = "Aisle",
    y = "Number of items"
  )
```

* We can see from the plot that among aisles with more than 10000 items ordered, `fresh vegetables` is the aisle with most ordered items, and `butter` is the aisle with the least ordered items.  
* The number of items ordered in `fresh vegetables` and `fresh fruits` aisles is around 150,000, which is generally more than two times of most other aisles.

```{r}
# Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarize(order_times = n()) %>% 
  mutate(order_rank = min_rank(desc(order_times))) %>% 
  filter(order_rank <= 3) %>% 
  arrange(aisle, -order_times) %>% 
  knitr::kable()
```

```{r}
# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = round(mean(order_hour_of_day, na.rm = T), 2)) %>% 
  mutate(order_dow = replace(order_dow, order_dow == c("0", "1", "2", "3", "4", "5", "6"), c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))) %>% 
  pivot_wider(names_from = order_dow,
              values_from = mean_hour) %>% 
  relocate(product_name, Mon, Tue, Wed, Thu, Fri, Sat, Sun) %>% 
  knitr::kable()
```



# Problem 2

```{r}
# Load BRFSS data and do some cleaning.

data("brfss_smart2010")

brfss_smart2010 = janitor::clean_names(brfss_smart2010) %>% 
  rename(state = locationabbr) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response)) %>% 
  mutate(response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  arrange(response)
```

```{r}
# In 2002, which states were observed at 7 or more locations? What about in 2010?

loc_num_2002 = brfss_smart2010 %>% 
  filter(year == "2002") %>% 
  distinct(state, locationdesc) %>% 
  group_by(state) %>% 
  summarize(locations_number = n()) %>% 
  filter(locations_number >= 7) %>% 
  arrange(-locations_number)

knitr::kable(loc_num_2002)

loc_num_2010 = brfss_smart2010 %>% 
  filter(year == "2010") %>%
  distinct(state, locationdesc) %>% 
  group_by(state) %>% 
  summarize(locations_number = n()) %>% 
  filter(locations_number >= 7) %>% 
  arrange(-locations_number)

knitr::kable(loc_num_2010)
```

* In 2002, there are `r nrow(loc_num_2002)` states were observed at 7 or more locations, which are : `r pull(loc_num_2002, state)`.  
* In 2010, there are`r nrow(loc_num_2010)` states were observed at 7 or more locations, which are : `r pull(loc_num_2010, state)`. 

```{r}
# Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state.

brfss_excellent = brfss_smart2010 %>% 
  filter(response == "Excellent") %>% 
  select(year, state, locationdesc, data_value) %>% 
  drop_na() %>% 
  group_by(year, state) %>% 
  mutate(mean_value = mean(data_value))

# Make a “spaghetti” plot of this average value over time within a state.

brfss_excellent_plot = ggplot(brfss_excellent, aes(x = year, y = mean_value)) +
  geom_line(aes(group = state, color = state)) +
  labs(
    title = "Average value over time within a state",
    x = "Year",
    y = "Average value"
  ) +
  theme(legend.position = "right")

brfss_excellent_plot
```

```{r}
# Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

brfss_ny_plot = brfss_smart2010 %>% 
  filter(year %in% c("2006", "2010"), state == "NY") %>% 
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .8) +
  facet_grid(. ~ year) +
  labs(
    title = "Response value in New York state",
    x = "Response value",
    y = "Density"
  )

brfss_ny_plot
```



# Problem 3

```{r}
# Import and clean accel_data.csv.

accel = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names()

accel = 
  mutate(accel, day = factor(day)) %>% 
  mutate(weekday_vs_weekend = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")) %>% 
  mutate(weekday_vs_weekend = factor(weekday_vs_weekend)) %>% 
  relocate(week, day_id, day, weekday_vs_weekend)
```

* There are `r nrow(accel)` observations and `r ncol(accel)` variables in `accel` dataset. The variable names are: `week`, `day_id`, `day`, `weekday_vs_weekend` and `activity.*`. Among them, variables `activity.*` are the activity counts for each minute of a 24-hour day starting at midnight.

```{r}
# Aggregate across minutes to create a total activity variable for each day, and create a table showing these totals.

accel_longer = pivot_longer(
  data = accel,
  activity_1:activity_1440,
  names_to =  "minute",
  names_prefix = "activity_",
  values_to = "activity"
)

accel_summarize = accel_longer %>% 
  group_by(day_id) %>% 
  summarize(total_activity = sum(activity))

knitr::kable(accel_summarize)
```

* The activity in the middle of the week seems lower than other days.  
* Most "active" days (with a higher activity count) are followed with one or more "inactive" (with a lower activity count) days.  


```{r}
#  Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

activity_plot = accel_longer %>%
  mutate(minute = as.numeric(minute)) %>% 
  ggplot(aes(x = minute, y = activity, color = day, group = day_id)) +
  geom_line(alpha = .5) +
  labs(title = "Activity of the day",
       x = "Time",
       y = "Activity count",
       ) +
  scale_x_continuous(breaks = c(seq(from = 0, to = 1440, by = 360)),
                   labels = c("12:00 am", "6:00 am", "12:00 pm", "6:00 pm", "11:59 pm"))

activity_plot
```

* Based on the color of the plot, we can see that Wednesday has a low activity level in general; the activity levels on Friday and Sunday are relatively high.  
* Throughout the course of a day, the most active time periods are noon and late night, because the peaks of the plot are more condense in these periods.  
* Throughout the course of a day, the least active time period is between 12:00 am and 6:00 am, because the heights of the plot are lower than those in other time periods.  
* The highest active count is on a Wednesday, around 8:00 pm.